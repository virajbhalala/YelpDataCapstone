{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "import re\n",
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import re  # For preprocessing\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hygiene_text_path= \"../data/Hygiene/hygiene.dat\"\n",
    "hygiene_labels_path= \"../data/Hygiene/hygiene.dat.labels\"\n",
    "hygiene_others_path= \"../data/Hygiene/hygiene.dat.additional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hygiene_text_path) as f:\n",
    "    arrText = [l.rstrip() for l in f]\n",
    "with open(hygiene_labels_path) as f:\n",
    "    arrLabels = [l.rstrip() for l in f]\n",
    "\n",
    "df = pd.DataFrame({'text':arrText, 'labels':arrLabels})\n",
    "hygiene_others = pd.read_csv(hygiene_others_path, names=[\"cuisines\", \"zipcode\", \"reviews\", \"avg_ratings\"])\n",
    "df = df.join(hygiene_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.cuisines = [ast.literal_eval(x) for x in df.cuisines]\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "res = pd.DataFrame(mlb.fit_transform(df.cuisines),\n",
    "                   columns=mlb.classes_,\n",
    "                   index=df.cuisines.index)\n",
    "df = df.drop(\"cuisines\", axis =1)\n",
    "df = df.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model without using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"labels\"] != \"[None]\" ]\n",
    "test_df = df[df[\"labels\"] == \"[None]\" ]\n",
    "X_train, y_train =train_df.drop(['text', 'labels', \"zipcode\"], axis=1), train_df[\"labels\"]\n",
    "X_test, y_test =test_df.drop(['text', 'labels', \"zipcode\"], axis=1), test_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# dtrain = xgb.DMatrix(np.array(X_train), label=np.array(y_train))\n",
    "# dtest = xgb.DMatrix(np.array(X_test))\n",
    "# param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
    "# param['nthread'] = 4\n",
    "# param['eval_metric'] = 'auc'\n",
    "# bst = xgb.train(param, dtrain, 10)\n",
    "# y_pred = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(np.array(X_train), np.array(y_train))\n",
    "y_pred = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./baseline_predictions.out', y_pred, fmt='%s')\n",
    "with open('./baseline_predictions.out', 'r') as original: data = original.read()\n",
    "with open('./baseline_predictions.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F1: 0.6659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "    \n",
    "    \n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilWordEmbedding import DocPreprocess\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "all_docs = DocPreprocess(nlp, stop_words, df['text'], df['labels'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dir_path = \"./\"\n",
    "# Save all_docs as pickle.\n",
    "with open(os.path.join(dir_path, 'all_docs.pickle'), 'wb') as f:\n",
    "    pickle.dump(all_docs, f, pickle.HIGHEST_PROTOCOL)\n",
    "# Read pickle.\n",
    "with open(os.path.join(dir_path, 'all_docs.pickle'), 'rb') as f:\n",
    "    all_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_docs.tagdocs), df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word embedding using Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = multiprocessing.cpu_count()\n",
    "word_model = Word2Vec(all_docs.doc_words,\n",
    "                      min_count=2,\n",
    "                      size=100,\n",
    "                      window=5,\n",
    "                      workers=workers,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.wv.syn0[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## averaging word embedding in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilWordEmbedding import MeanEmbeddingVectorizer\n",
    "\n",
    "mean_vec_tr = MeanEmbeddingVectorizer(word_model)\n",
    "doc_vec = mean_vec_tr.transform(all_docs.doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(dir_path,'doc_vec.csv'), doc_vec, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding_df = df.join(pd.DataFrame(doc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = mean_embedding_df[mean_embedding_df[\"labels\"] != \"[None]\" ]\n",
    "test_df = mean_embedding_df[mean_embedding_df[\"labels\"] == \"[None]\" ]\n",
    "X_train, y_train =train_df.drop(['text', 'labels', 'zipcode'], axis=1), train_df[\"labels\"]\n",
    "X_test, y_test =test_df.drop(['text', 'labels', 'zipcode'], axis=1), test_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(np.array(X_train), label=np.array(y_train))\n",
    "dtest = xgb.DMatrix(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=100, subsample=1, colsample_bytree=1, colsample_bylevel=1)\n",
    "model.fit(np.array(X_train), np.array(y_train))\n",
    "y_pred = model.predict(np.array(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 6, 'eta': 0.3, 'objective': 'binary:logistic', 'subsample':0.8, \"n_estimators\":200}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "bst = xgb.train(param, dtrain)\n",
    "y_pred = bst.predict(dtest)\n",
    "y_pred = np.where(y_pred > 0.95, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./average_word2vec_predictions.out', y_pred, fmt='%s')\n",
    "with open('./average_word2vec_predictions.out', 'r') as original: data = original.read()\n",
    "with open('./average_word2vec_predictions.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F1: 0.7027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=201, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer= \"random_uniform\", bias_initializer='zeros'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(np.array(X_train, dtype=np.float32),np.array(y_train, dtype=np.float32) , epochs=100, batch_size=32)\n",
    "y_pred = model.predict(np.array(X_test, dtype=np.float32))\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "np.savetxt('./average_word2vec_predictions_dl.out', y_pred, fmt='%s')\n",
    "with open('./average_word2vec_predictions_dl.out', 'r') as original: data = original.read()\n",
    "with open('./average_word2vec_predictions_dl.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilWordEmbedding import TfidfEmbeddingVectorizer\n",
    "tfidf_vec_tr = TfidfEmbeddingVectorizer(word_model)\n",
    "\n",
    "tfidf_vec_tr.fit(all_docs.doc_words)  # fit tfidf model first\n",
    "tfidf_doc_vec = tfidf_vec_tr.transform(all_docs.doc_words)\n",
    "np.savetxt(os.path.join(dir_path, './tfidf_doc_vec.csv'), tfidf_doc_vec, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mean_embedding_df = df.join(pd.DataFrame(tfidf_doc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tfidf_mean_embedding_df[tfidf_mean_embedding_df[\"labels\"] != \"[None]\" ]\n",
    "test_df = tfidf_mean_embedding_df[tfidf_mean_embedding_df[\"labels\"] == \"[None]\" ]\n",
    "X_train, y_train =train_df.drop(['text', 'labels', 'zipcode'], axis=1), train_df[\"labels\"]\n",
    "X_test, y_test =test_df.drop(['text', 'labels', 'zipcode'], axis=1), test_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=201, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer= \"random_uniform\", bias_initializer='zeros'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(np.array(X_train, dtype=np.float32),np.array(y_train, dtype=np.float32) , epochs=100, batch_size=64)\n",
    "y_pred = model.predict(np.array(X_test, dtype=np.float32))\n",
    "\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "np.savetxt('./average_word2vec_predictions_dl.out', y_pred, fmt='%s')\n",
    "with open('./average_word2vec_predictions_dl.out', 'r') as original: data = original.read()\n",
    "with open('./average_word2vec_predictions_dl.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
