{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Sometimes we make decisions beyond the rating of a restaurant. For example, if a restaurant has a high rating but it often fails to pass hygiene inspections, then this information can dissuade many people to eat there. Using this hygiene information could lead to a more informative system; however, it is often the case where we donâ€™t have such information for all the restaurants, and we are left to make predictions based on the small sample of data points.\n",
    "\n",
    "In this task, we are going to predict whether a set of restaurants will pass the public health inspection tests given the corresponding Yelp text reviews along with some additional information such as the locations and cuisines offered in these restaurants. Making a prediction about an unobserved attribute using data mining techniques represents a wide range of important applications of data mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  \n",
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy  \n",
    "import re  \n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Dataset\n",
    "This dataset can be obtained from: https://d396qusza40orc.cloudfront.net/dataminingcapstone/Task6/Hygiene.tar.gz\n",
    "\n",
    "The dataset is composed of a training subset containing 546 restaurants used for training your classifier, in addition to a testing subset of 12753 restaurants used for evaluating the performance of the classifier. In the training subset, we have a binary label for each restaurant, which indicates whether the restaurant has passed the latest public health inspection test or not, whereas for the testing subset, we will not have access to any labels. The dataset is spread across three files such that the first 546 lines in each file corresponding to the training subset, and the rest are part of the testing subset. Below is a description of each file:\n",
    "\n",
    "- hygiene.dat: Each line contains the concatenated text reviews of one restaurant.\n",
    "- hygiene.dat.labels: For the first 546 lines, a binary label (0 or 1) is used where a 0 indicates that the restaurant has passed the latest public health inspection test, while a 1 means that the restaurant has failed the test. The rest of the lines have \"[None]\" in their label field implying that they are part of the testing subset.\n",
    "- hygiene.dat.additional: It is a CSV (Comma-Separated Values) file where the first value is a list containing the cuisines offered, the second value is the zip code, which gives an idea about the location, the third is the number of reviews, and the fourth is the average rating, which can vary between 0 and 5 (5 being the best).\n",
    "\n",
    "\n",
    "\n",
    "Other Notes:\n",
    "- The training data is perfectly balanced, whereas the testing data is skewed, which creates a new challenge since the training and testing data have different distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About test data and scoring test data labels\n",
    "\n",
    "- Students cannot access labels of test dataset. In order to check f1 score of a model, call the scoring API. Please see submit.py file for more info. To get F1 score in leaderboard, call **python submit.py [ucid] [filename of file that contains test labels]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data is provided by course instructor. Please see above note on how to access it.\n",
    "hygiene_text_path= \"../data/Hygiene/hygiene.dat\"\n",
    "hygiene_labels_path= \"../data/Hygiene/hygiene.dat.labels\"\n",
    "hygiene_others_path= \"../data/Hygiene/hygiene.dat.additional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hygiene_text_path) as f:\n",
    "    arrText = [l.rstrip() for l in f]\n",
    "with open(hygiene_labels_path) as f:\n",
    "    arrLabels = [l.rstrip() for l in f]\n",
    "df = pd.DataFrame({'text':arrText, 'labels':arrLabels})\n",
    "hygiene_others = pd.read_csv(hygiene_others_path, names=[\"cuisines\", \"zipcode\", \"reviews\", \"avg_ratings\"])\n",
    "df = df.join(hygiene_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create OHE of different cuisine types. MultiLabelBinarizer is used to transform arrays into encodings\n",
    "df.cuisines = [ast.literal_eval(x) for x in df.cuisines]\n",
    "mlb = MultiLabelBinarizer()\n",
    "res = pd.DataFrame(mlb.fit_transform(df.cuisines),\n",
    "                   columns=mlb.classes_,\n",
    "                   index=df.cuisines.index)\n",
    "df = df.drop(\"cuisines\", axis =1)\n",
    "df = df.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is any NAs\n",
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model without using NLP \n",
    "Results of this base model will help to compare other advance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"labels\"] != \"[None]\" ]\n",
    "test_df = df[df[\"labels\"] == \"[None]\" ]\n",
    "X_train, y_train =train_df.drop(['text', 'labels', \"zipcode\"], axis=1), train_df[\"labels\"]\n",
    "X_test, y_test =test_df.drop(['text', 'labels', \"zipcode\"], axis=1), test_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(np.array(X_train), np.array(y_train))\n",
    "y_pred = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./baseline_predictions.out', y_pred, fmt='%s')\n",
    "with open('./baseline_predictions.out', 'r') as original: data = original.read()\n",
    "with open('./baseline_predictions.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F1: 0.6659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw text preprocesing of reviews for more advanced model\n",
    "- lemmatization, lower case, removing stop words, regex processing,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "    \n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilWordEmbedding import DocPreprocess\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "all_docs = DocPreprocess(nlp, stop_words, df['text'], df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = \"./\"\n",
    "# # Save all_docs as pickle.\n",
    "with open(os.path.join(dir_path, 'all_docs.pickle'), 'wb') as f:\n",
    "    pickle.dump(all_docs, f, pickle.HIGHEST_PROTOCOL)\n",
    "# Read pickle.\n",
    "with open(os.path.join(dir_path, 'all_docs.pickle'), 'rb') as f:\n",
    "    all_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13299, (13299, 104))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs.tagdocs), df.shape # check whether dimension is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build word embedding using Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = multiprocessing.cpu_count()\n",
    "word_model = Word2Vec(all_docs.doc_words,\n",
    "                      min_count=2,\n",
    "                      size=100,\n",
    "                      window=5,\n",
    "                      workers=workers,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.wv.syn0[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging word embedding for each restuarant's reviews(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilWordEmbedding import MeanEmbeddingVectorizer\n",
    "mean_vec_tr = MeanEmbeddingVectorizer(word_model)\n",
    "doc_vec = mean_vec_tr.transform(all_docs.doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(dir_path,'doc_vec.csv'), doc_vec, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec = pd.read_csv(\"./doc_vec.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding_df = df.join(pd.DataFrame(doc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13299, 204)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embedding_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = mean_embedding_df[mean_embedding_df[\"labels\"] != \"[None]\" ]\n",
    "test_df = mean_embedding_df[mean_embedding_df[\"labels\"] == \"[None]\" ]\n",
    "X_train, y_train =train_df.drop(['text', 'labels', 'zipcode'], axis=1), train_df[\"labels\"]\n",
    "X_test, y_test =test_df.drop(['text', 'labels', 'zipcode'], axis=1), test_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(np.array(X_train), label=np.array(y_train))\n",
    "dtest = xgb.DMatrix(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=100, subsample=1, colsample_bytree=1, colsample_bylevel=1)\n",
    "model.fit(np.array(X_train), np.array(y_train))\n",
    "y_pred = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 6, 'eta': 0.3, 'objective': 'binary:logistic', 'subsample':0.8, \"n_estimators\":200}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "bst = xgb.train(param, dtrain)\n",
    "y_pred = bst.predict(dtest)\n",
    "y_pred = np.where(y_pred > 0.95, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./average_word2vec_predictions.out', y_pred, fmt='%s')\n",
    "with open('./average_word2vec_predictions.out', 'r') as original: data = original.read()\n",
    "with open('./average_word2vec_predictions.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F1: 0.7027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Feed Forward Neural Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom supporting functions for keras model\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=201, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer= \"random_uniform\", bias_initializer='zeros'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(np.array(X_train, dtype=np.float32),np.array(y_train, dtype=np.float32) , epochs=100, batch_size=32)\n",
    "y_pred = model.predict(np.array(X_test, dtype=np.float32))\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "np.savetxt('./average_word2vec_predictions_dl.out', y_pred, fmt='%s')\n",
    "with open('./average_word2vec_predictions_dl.out', 'r') as original: data = original.read()\n",
    "with open('./average_word2vec_predictions_dl.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** - results are not that great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(np.array(X_test.fillna(0), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./average_word2vec_predictions_lr.out', y_pred, fmt='%s')\n",
    "with open('./average_word2vec_predictions_lr.out', 'r') as original: data = original.read()\n",
    "with open('./average_word2vec_predictions_lr.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency- Inverse Document Frequency & Averaging Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilWordEmbedding import TfidfEmbeddingVectorizer\n",
    "tfidf_vec_tr = TfidfEmbeddingVectorizer(word_model)\n",
    "\n",
    "tfidf_vec_tr.fit(all_docs.doc_words)  # fit tfidf model first\n",
    "tfidf_doc_vec = tfidf_vec_tr.transform(all_docs.doc_words)\n",
    "np.savetxt(os.path.join(dir_path, './tfidf_doc_vec.csv'), tfidf_doc_vec, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mean_embedding_df = df.join(pd.DataFrame(tfidf_doc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tfidf_mean_embedding_df[tfidf_mean_embedding_df[\"labels\"] != \"[None]\" ]\n",
    "test_df = tfidf_mean_embedding_df[tfidf_mean_embedding_df[\"labels\"] == \"[None]\" ]\n",
    "X_train, y_train =train_df.drop(['text', 'labels', 'zipcode'], axis=1), train_df[\"labels\"]\n",
    "X_test, y_test =test_df.drop(['text', 'labels', 'zipcode'], axis=1), test_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=201, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='linear', kernel_initializer= \"random_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer= \"random_uniform\", bias_initializer='zeros'))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"binary_accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(np.array(X_train, dtype=np.float32),np.array(y_train, dtype=np.float32) , epochs=100, batch_size=64)\n",
    "y_pred = model.predict(np.array(X_test, dtype=np.float32))\n",
    "\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "np.savetxt('./average_word2vec_predictions_dl.out', y_pred, fmt='%s')\n",
    "with open('./average_word2vec_predictions_dl.out', 'r') as original: data = original.read()\n",
    "with open('./average_word2vec_predictions_dl.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizor using sklearn \n",
    "Results are very poor with logistic regression compare to models that uses word2vec features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmer=nltk.stem.WordNetLemmatizer()\n",
    "new_corpus=[' '.join([lemmer.lemmatize(word) for word in text.split(' ')])\n",
    "          for text in df[df[\"labels\"] != \"[None]\" ]['text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=set(nltk.corpus.stopwords.words('english')) ,\n",
    "#                   ngram_range=(2, 5),\n",
    "#                   token_pattern = ' \\b[^\\d\\W]+\\b' #tokenizer overwrites this so see above lemma class. This match all words excepts ones that have numbers in them. This also ignores any other chars like punctuations\n",
    "                      )  \n",
    "X = vect.fit_transform(new_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = df.join(pd.DataFrame(X.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = countVec[countVec[\"labels\"] != \"[None]\" ]\n",
    "test_df = countVec[countVec[\"labels\"] == \"[None]\" ]\n",
    "X_train, y_train =train_df.drop(['text', 'labels', 'zipcode'], axis=1), train_df[\"labels\"]\n",
    "X_test, y_test =test_df.drop(['text', 'labels', 'zipcode'], axis=1), test_df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(np.array(X_test.fillna(0), dtype=np.float32))\n",
    "np.savetxt('./bow_predictions_lr.out', y_pred, fmt='%s')\n",
    "with open('./bow_predictions_lr.out', 'r') as original: data = original.read()\n",
    "with open('./bow_predictions_lr.out', 'w') as modified: modified.write(\"Viraj Bhalala(vbb2)\\n\" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "Baseline model\n",
    "![Image ](./img/baselineModel.png )\n",
    "\n",
    "word2vev doc average - xgboost\n",
    "![Image ](./img/word2vecAvgxgboost.png )\n",
    "\n",
    "\n",
    "word2vev doc average - Feed Forward Neural Net\n",
    "![Image ](./img/word2vecAvgDL.png )\n",
    "\n",
    "\n",
    "word2vev doc average & TF-IDF  - Feed Forward Neural Net\n",
    "![Image ](./img/word2vecTFIDFDL.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "After running various experiments, we can see that using doc average of word2vec embedding of all words in a doc provided us decent F1 score. In addition, we also found out that Feed Forward Neural Network performed quite well compare to other models in above experiments. One of the reason Neural Net did well is because we were able to customize it based on our requirements. In particular, increasing dropout layer percentage after each layer generated quite higher F1 score in test set. Moreover, we found out that dropout percentage of around 50% was optimal. We tested smaller and large percentage but resutls were not that great. One of the most challenging part of this project was to avoid overfitting training set. As you can see above, we have very small training set compare to test set, moreover training set consist only 4% of overall dataset. It is very important that we train model on training set very carefully to avoid any overfitting. Most of my early modelling attempts overfit the training set. These attempts included training xgb classifier and neural net with no or very small percentage of drop out layer(results of these models are not shown above as they were pretty bad). We also found out from these experiments that word2vec and word2vec+tf idf did quite well to generalize data and find great relationship between different review documents compare to count vectorizor(bag of words). Results from BOW on training logistic regression was very bad(not show above), this is because BOW generated very sparse feature matrix which again overfits the training data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How can we improve this in future?\n",
    "\n",
    "There are couple of ways we can improve the results. First of all, we can put more effort to create better modelling pipeline that can include grid search to find best paramenters of different models. One the main challenge in this part is that we have to incorporate API that instructor created in our code to automatically get F1 score of test set after each attempts. The second challenge is that it doesnt make sense to further split train set into train and validation set since train set is such as small set. In addition, we are not allowed to acces test set. Finally, after having a good modelling pipleine, we can test other text processing methods such as doc2vec, LDA, tf-idf, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
